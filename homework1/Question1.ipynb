{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、Pytorch基本操作考察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用torch库\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#调用numpy库\n",
    "import numpy as np\n",
    "#调用matplotlib库\n",
    "import matplotlib.pyplot as plt\n",
    "#调用random库\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1. 使用 𝐓𝐞𝐧𝐬𝐨𝐫 初始化一个 𝟏 × 𝟑 的矩阵 𝑴 和一个 𝟐 × 𝟏 的矩阵 𝑵，对两矩阵进行减法操作（要求\n",
    "实现三种不同的形式），给出结果并分析三种方式的不同（如果出现报错，分析报错的原因），\n",
    "同时需要指出在计算过程中发生了什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) torch.Size([2, 1])\n",
      "tensor([[0.4942, 0.2690, 0.1761]])\n",
      "tensor([[0.4602],\n",
      "        [0.2456]])\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand(1, 3)\n",
    "N = torch.rand(2, 1)\n",
    "print(M.shape, N.shape)\n",
    "print(M)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形式1 直接相加\n",
    "此处使用了tensor的广播机制，二者自动复制成为相同的shape进行运算，因此可以正常运行不会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[0.9545, 0.7292, 0.6364],\n",
      "        [0.7399, 0.5147, 0.4218]])\n"
     ]
    }
   ],
   "source": [
    "Add = M + N\n",
    "print(Add.shape)\n",
    "print(Add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形式2 使用Pytorch提供的加和函数进行运算，原理同上\n",
    "依然使用了tensor的广播机制，二者自动复制成为相同的shape进行运算，因此可以正常运行不会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[0.9545, 0.7292, 0.6364],\n",
      "        [0.7399, 0.5147, 0.4218]])\n"
     ]
    }
   ],
   "source": [
    "Add = torch.add(M, N)\n",
    "print(Add.shape)\n",
    "print(Add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形式3 使用tensor自身的inplace操作\n",
    "这一步不会进行新tensor的产生，也不能在原tensor的内存空间上进行广播，因此要求两tensor具备相同的shape，shape不同则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 3] doesn't match the broadcast shape [2, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d9007d7efe05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 3] doesn't match the broadcast shape [2, 3]"
     ]
    }
   ],
   "source": [
    "Add = M.add_(N)\n",
    "print(Add.shape)\n",
    "print(Add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题2-1. 利用 𝐓𝐞𝐧𝐬𝐨𝐫 创建两个大小分别 𝟑 × 𝟐 和 𝟒 × 𝟐 的随机数矩阵 𝑷 和 𝑸 ，要求服从均值为0，标\n",
    "准差0.01为的正态分布；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([4, 2])\n",
      "tensor([[-0.0045,  0.0189],\n",
      "        [ 0.0087, -0.0015],\n",
      "        [ 0.0048, -0.0021]])\n",
      "tensor([[-0.0022,  0.0085],\n",
      "        [-0.0053,  0.0062],\n",
      "        [ 0.0136, -0.0009],\n",
      "        [-0.0035, -0.0029]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.normal(mean=torch.full((3,2),0.0),std=torch.full((3,2),0.01))\n",
    "Q = torch.normal(mean=torch.full((4,2),0.0),std=torch.full((4,2),0.01))\n",
    "print(P.shape, Q.shape)\n",
    "print(P)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题2-2. 对第二步得到的矩阵 𝑸 进行形状变换得到 𝑸 的转置；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "tensor([[-0.0022, -0.0053,  0.0136, -0.0035],\n",
      "        [ 0.0085,  0.0062, -0.0009, -0.0029]])\n"
     ]
    }
   ],
   "source": [
    "Q = Q.t()\n",
    "print(Q.shape)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题2-3. 对上述得到的矩阵 𝑷 和矩阵 𝑸𝑻 求内积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[ 1.7130e-04,  1.4187e-04, -7.8602e-05, -3.9020e-05],\n",
      "        [-3.2270e-05, -5.5599e-05,  1.2048e-04, -2.5948e-05],\n",
      "        [-2.8690e-05, -3.8415e-05,  6.6795e-05, -1.0365e-05]])\n"
     ]
    }
   ],
   "source": [
    "R = torch.mm(P, Q)\n",
    "print(R.shape)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题3. 给定公式 𝑦3 = 𝑦1 + 𝑦2 = 𝑥\n",
    "2 + 𝑥\n",
    "3\n",
    "，且 𝑥 = 1。利用学习所得到的Tensor的相关知识，求𝑦3对𝑥的\n",
    "梯度，即𝑑𝑦3\n",
    "𝑑𝑥 。\n",
    "要求在计算过程中，在计算 𝑥\n",
    "3 时中断梯度的追踪，观察结果并进行原因分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先在第一次尝试中不中断y2的梯度追踪使torch的自动梯度追踪能够正常运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.])\n",
    "x = torch.autograd.Variable(x,requires_grad=True)\n",
    "y1 = torch.pow(x, 2)\n",
    "y2 = torch.pow(x, 3)\n",
    "y3 = y1 + y2\n",
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其次在第二次尝试中中断y2的梯度追踪，该尝试会导致梯度回传计算错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.])\n",
    "x = torch.autograd.Variable(x,requires_grad=True)\n",
    "y1 = torch.pow(x, 2)\n",
    "with torch.no_grad():\n",
    "    y2 = torch.pow(x, 3)\n",
    "y3 = y1 + y2\n",
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该结果与第一次的计算出现差异，原因是y2变量的梯度计算未包含在内。而该变量的梯度值为3，为两次y3梯度值的计算差异。这证明了中断中间变量的梯度计算会导致最终的梯度计算出现问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb47ef1ad79943d811bafa2fa84fcb47147f85a402b583945e5888afc6d949fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
